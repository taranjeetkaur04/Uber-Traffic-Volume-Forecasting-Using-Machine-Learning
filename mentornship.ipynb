{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed29f705-0e44-4add-b24e-25fbdf9624ef",
   "metadata": {},
   "source": [
    "## STEP 1:Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b243d9d5-5ba4-4066-8c5a-c47f1cd16a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48120 entries, 0 to 48119\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   DateTime  48120 non-null  object\n",
      " 1   Junction  48120 non-null  int64 \n",
      " 2   Vehicles  48120 non-null  int64 \n",
      " 3   ID        48120 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DateTime    0\n",
       "Junction    0\n",
       "Vehicles    0\n",
       "ID          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Dataset_Uber Traffic.csv')\n",
    "\n",
    "# Preview\n",
    "df.head()\n",
    "df.info()\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bab243-4f67-42bf-b83d-4bcfd7e55c8a",
   "metadata": {},
   "source": [
    "## STEP 2: Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "188bd8fb-bab7-44a9-b7e2-edd30c34c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39092c30-8714-4de9-afe7-e275534ea7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime    0\n",
       "Junction    0\n",
       "Vehicles    0\n",
       "ID          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97bbaf6e-7e2c-48c6-9c97-fff9b37e7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'Vehicles' column with median value\n",
    "df['Vehicles'] = df['Vehicles'].fillna(df['Vehicles'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92619565-227e-41fe-b293-0ef91ee4b347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateTime    datetime64[ns]\n",
       "Junction             int64\n",
       "Vehicles             int64\n",
       "ID                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df82db3e-83e1-4ad0-9f50-792ed98f2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DateTime column to proper datetime format\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41661234-92e5-463d-8fb8-f4d7bf95f578",
   "metadata": {},
   "source": [
    "## STEP 3: Aggregate traffic data\n",
    "✅ STEP 1: Load & Prepare Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27da983e-d296-433e-81c3-4ffb90df8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"Dataset_Uber Traffic.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0055ea-4049-4364-8965-deceb79a7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# Extract useful time components\n",
    "df['Date'] = df['DateTime'].dt.date\n",
    "df['Hour'] = df['DateTime'].dt.hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd014ef5-88d5-48c5-9b27-e07ce2e704f4",
   "metadata": {},
   "source": [
    "\n",
    "✅ STEP 2: Group by Junction, Date, and Hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "811df601-7d8d-4887-ad3a-a54eaf18cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group to get hourly traffic volume per junction\n",
    "hourly_df = df.groupby(['Junction', 'Date', 'Hour'])['Vehicles'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9861b5f1-1e79-4c3c-851c-950a70abe72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2015-11-01 00:00:00\n",
      "1   2015-11-01 01:00:00\n",
      "2   2015-11-01 02:00:00\n",
      "3   2015-11-01 03:00:00\n",
      "4   2015-11-01 04:00:00\n",
      "5   2015-11-01 05:00:00\n",
      "6   2015-11-01 06:00:00\n",
      "7   2015-11-01 07:00:00\n",
      "8   2015-11-01 08:00:00\n",
      "9   2015-11-01 09:00:00\n",
      "Name: DateTime, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#Quick test to check formats:\n",
    "print(df['DateTime'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf80d44-e8ba-4a3f-bb78-ae4cb3e38e26",
   "metadata": {},
   "source": [
    "## STEP 4: Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad646c2d-6e7a-4b73-afdf-84926d237d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll go with Standardization (z-score), as it's more robust for time series with outliers.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume you already have hourly_df from the last step\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Only scale the 'Vehicles' column\n",
    "hourly_df['Vehicles_scaled'] = scaler.fit_transform(hourly_df[['Vehicles']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff17b7cf-bbe3-48db-8f71-0d6bfeff96f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   Junction | Date       |   Hour |   Vehicles |   Vehicles_scaled |\n",
      "|----|------------|------------|--------|------------|-------------------|\n",
      "|  0 |          1 | 2015-01-11 |      0 |         15 |         -0.375489 |\n",
      "|  1 |          1 | 2015-01-11 |      1 |         13 |         -0.471875 |\n",
      "|  2 |          1 | 2015-01-11 |      2 |         10 |         -0.616454 |\n",
      "|  3 |          1 | 2015-01-11 |      3 |          7 |         -0.761034 |\n",
      "|  4 |          1 | 2015-01-11 |      4 |          9 |         -0.664648 |\n"
     ]
    }
   ],
   "source": [
    "# Aiming to view the scaled vehicle data in a clean tabular format\n",
    "from tabulate import tabulate\n",
    "print(tabulate(hourly_df[['Junction', 'Date', 'Hour', 'Vehicles', 'Vehicles_scaled']].head(), headers='keys', tablefmt='github'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8157471-d876-40c9-a5ff-b067ece480b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the Preprocessed Data\n",
    "hourly_df.to_csv(\"Preprocessed_Traffic_Data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd33419-78ed-4d04-96b8-9520a5f0fad8",
   "metadata": {},
   "source": [
    "## Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f154a-1829-4ac4-98d3-6a1aefc9357a",
   "metadata": {},
   "source": [
    "✅ STEP 1: Create New Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0c20b47-bb41-428c-9dc9-aa7246a202bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df['DateTime'] = pd.to_datetime(hourly_df['Date'].astype(str) + ' ' + hourly_df['Hour'].astype(str) + ':00:00')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b288f35b-e876-427e-9845-35581147e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DateTime if needed\n",
    "hourly_df['DateTime'] = pd.to_datetime(hourly_df['DateTime'])\n",
    "# Extract time-based features\n",
    "hourly_df['Hour'] = hourly_df['DateTime'].dt.hour\n",
    "hourly_df['DayOfWeek'] = hourly_df['DateTime'].dt.dayofweek  # Monday = 0\n",
    "hourly_df['Month'] = hourly_df['DateTime'].dt.month\n",
    "\n",
    "# Weekend binary feature\n",
    "hourly_df['IsWeekend'] = hourly_df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "358e797a-c03b-4977-a73e-1aca3193ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Junction', 'Date', 'Hour', 'Vehicles', 'Vehicles_scaled', 'DateTime',\n",
      "       'DayOfWeek', 'Month', 'IsWeekend'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(hourly_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb861ad-5226-497e-85d2-9ee878a5ddc3",
   "metadata": {},
   "source": [
    "✅ STEP 2: Create Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67b952ae-d2ae-4c50-b650-3d025e896dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag traffic count by 1 and 2 hours\n",
    "hourly_df['Lag_1'] = hourly_df.groupby('Junction')['Vehicles'].shift(1)\n",
    "hourly_df['Lag_2'] = hourly_df.groupby('Junction')['Vehicles'].shift(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fadf5d-4edc-493f-8bcc-fb49af959c8c",
   "metadata": {},
   "source": [
    "✅ STEP 3: Add Special Event Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cbcb1545-6a98-47ca-9a69-d9bbf5ad0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: Assume no special events for now\n",
    "hourly_df['IsEvent'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13161c13-6096-403b-957f-e44cb16140e2",
   "metadata": {},
   "source": [
    "✅ STEP 4: Evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1ba2b594-7724-474f-b2ae-fcf6ee5b9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour: 0.0238\n",
      "DayOfWeek: 0.0065\n",
      "Month: 0.0095\n",
      "IsWeekend: 0.0011\n",
      "Lag_1: 0.9437\n",
      "Lag_2: 0.0154\n",
      "IsEvent: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop NaN rows (from lag features)\n",
    "hourly_df = hourly_df.dropna()\n",
    "\n",
    "# Define features and target\n",
    "features = ['Hour', 'DayOfWeek', 'Month', 'IsWeekend', 'Lag_1', 'Lag_2', 'IsEvent']\n",
    "X = hourly_df[features]\n",
    "y = hourly_df['Vehicles']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importances\n",
    "importances = rf.feature_importances_\n",
    "for feat, imp in zip(features, importances):\n",
    "    print(f\"{feat}: {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c514aa-b4fa-4b53-8da6-60f10aa59512",
   "metadata": {},
   "source": [
    "✅ STEP 5: Final Dataset for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cf6867f2-9744-46aa-9bd4-ad388858dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df.to_csv(\"Feature_Engineered_Traffic_Data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941a6d09-843f-40ea-90e6-1ad4add10ff8",
   "metadata": {},
   "source": [
    "## Industry Research: Impact of Traffic on Uber Fare Pricing\n",
    "\n",
    "#### Traffic significantly influences Uber's fare pricing due to dynamic pricing models like surge pricing. During peak hours, high demand and low driver availability lead to increased fares. Traffic congestion increases trip duration, which directly raises time-based charges. Events, public holidays, and bad weather also lead to traffic spikes, causing longer wait times and route detours — all of which raise fare estimates. This not only impacts customer cost but also affects Uber’s supply-demand balance and driver earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b6bb6-82a7-44f5-952f-ac99842424c5",
   "metadata": {},
   "source": [
    "## Data Collection and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "669995cb-cb32-4759-99ce-46f7d02a3b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'datetime', 'tempmax', 'tempmin', 'temp', 'feelslikemax',\n",
      "       'feelslikemin', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob',\n",
      "       'precipcover', 'preciptype', 'snow', 'snowdepth', 'windgust',\n",
      "       'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility',\n",
      "       'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'sunrise',\n",
      "       'sunset', 'moonphase', 'conditions', 'description', 'icon', 'stations'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#inspecting the actual column names\n",
    "print(weather_merged.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f4728-a91b-400d-a08d-76cb2356d5e2",
   "metadata": {},
   "source": [
    "## 🔹 WEATHER DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b736bcfb-77b6-4344-8729-5bd5ca9b7190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tempmax</th>\n",
       "      <th>tempmin</th>\n",
       "      <th>temp</th>\n",
       "      <th>feelslikemax</th>\n",
       "      <th>feelslikemin</th>\n",
       "      <th>feelslike</th>\n",
       "      <th>dew</th>\n",
       "      <th>humidity</th>\n",
       "      <th>...</th>\n",
       "      <th>solarenergy</th>\n",
       "      <th>uvindex</th>\n",
       "      <th>severerisk</th>\n",
       "      <th>sunrise</th>\n",
       "      <th>sunset</th>\n",
       "      <th>moonphase</th>\n",
       "      <th>conditions</th>\n",
       "      <th>description</th>\n",
       "      <th>icon</th>\n",
       "      <th>stations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhi, India</td>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>59.0</td>\n",
       "      <td>44.6</td>\n",
       "      <td>50.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>44.2</td>\n",
       "      <td>49.9</td>\n",
       "      <td>45.9</td>\n",
       "      <td>85.5</td>\n",
       "      <td>...</td>\n",
       "      <td>12.1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-11T07:15:19</td>\n",
       "      <td>2015-01-11T17:42:35</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>Partly cloudy throughout the day.</td>\n",
       "      <td>fog</td>\n",
       "      <td>42182099999,VIDP,42181099999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi, India</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>60.8</td>\n",
       "      <td>44.1</td>\n",
       "      <td>51.4</td>\n",
       "      <td>60.8</td>\n",
       "      <td>43.5</td>\n",
       "      <td>50.9</td>\n",
       "      <td>47.1</td>\n",
       "      <td>86.4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.2</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-12T07:15:18</td>\n",
       "      <td>2015-01-12T17:43:23</td>\n",
       "      <td>0.73</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>Partly cloudy throughout the day.</td>\n",
       "      <td>fog</td>\n",
       "      <td>42182099999,VIDP,42181099999,remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delhi, India</td>\n",
       "      <td>2015-01-13</td>\n",
       "      <td>63.4</td>\n",
       "      <td>42.8</td>\n",
       "      <td>51.6</td>\n",
       "      <td>63.4</td>\n",
       "      <td>41.1</td>\n",
       "      <td>50.9</td>\n",
       "      <td>47.4</td>\n",
       "      <td>86.8</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-13T07:15:16</td>\n",
       "      <td>2015-01-13T17:44:11</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>Partly cloudy throughout the day.</td>\n",
       "      <td>fog</td>\n",
       "      <td>42182099999,VIDP,42181099999,remote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delhi, India</td>\n",
       "      <td>2015-01-14</td>\n",
       "      <td>59.0</td>\n",
       "      <td>48.8</td>\n",
       "      <td>53.9</td>\n",
       "      <td>59.0</td>\n",
       "      <td>48.8</td>\n",
       "      <td>53.9</td>\n",
       "      <td>49.9</td>\n",
       "      <td>86.6</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-14T07:15:13</td>\n",
       "      <td>2015-01-14T17:45:00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>Rain, Partially cloudy</td>\n",
       "      <td>Partly cloudy throughout the day with rain.</td>\n",
       "      <td>rain</td>\n",
       "      <td>42182099999,VIDP,42181099999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delhi, India</td>\n",
       "      <td>2015-01-15</td>\n",
       "      <td>64.4</td>\n",
       "      <td>41.0</td>\n",
       "      <td>51.9</td>\n",
       "      <td>64.4</td>\n",
       "      <td>40.0</td>\n",
       "      <td>51.5</td>\n",
       "      <td>47.0</td>\n",
       "      <td>85.4</td>\n",
       "      <td>...</td>\n",
       "      <td>13.7</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-15T07:15:08</td>\n",
       "      <td>2015-01-15T17:45:49</td>\n",
       "      <td>0.83</td>\n",
       "      <td>Partially cloudy</td>\n",
       "      <td>Clearing in the afternoon.</td>\n",
       "      <td>fog</td>\n",
       "      <td>42182099999,VIDP,42181099999,remote</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name   datetime  tempmax  tempmin  temp  feelslikemax  \\\n",
       "0  Delhi, India 2015-01-11     59.0     44.6  50.5          59.0   \n",
       "1  Delhi, India 2015-01-12     60.8     44.1  51.4          60.8   \n",
       "2  Delhi, India 2015-01-13     63.4     42.8  51.6          63.4   \n",
       "3  Delhi, India 2015-01-14     59.0     48.8  53.9          59.0   \n",
       "4  Delhi, India 2015-01-15     64.4     41.0  51.9          64.4   \n",
       "\n",
       "   feelslikemin  feelslike   dew  humidity  ...  solarenergy  uvindex  \\\n",
       "0          44.2       49.9  45.9      85.5  ...         12.1        5   \n",
       "1          43.5       50.9  47.1      86.4  ...         12.2        6   \n",
       "2          41.1       50.9  47.4      86.8  ...         12.0        6   \n",
       "3          48.8       53.9  49.9      86.6  ...         10.6        5   \n",
       "4          40.0       51.5  47.0      85.4  ...         13.7        6   \n",
       "\n",
       "   severerisk              sunrise               sunset  moonphase  \\\n",
       "0         NaN  2015-01-11T07:15:19  2015-01-11T17:42:35       0.70   \n",
       "1         NaN  2015-01-12T07:15:18  2015-01-12T17:43:23       0.73   \n",
       "2         NaN  2015-01-13T07:15:16  2015-01-13T17:44:11       0.75   \n",
       "3         NaN  2015-01-14T07:15:13  2015-01-14T17:45:00       0.80   \n",
       "4         NaN  2015-01-15T07:15:08  2015-01-15T17:45:49       0.83   \n",
       "\n",
       "               conditions                                  description  icon  \\\n",
       "0        Partially cloudy            Partly cloudy throughout the day.   fog   \n",
       "1        Partially cloudy            Partly cloudy throughout the day.   fog   \n",
       "2        Partially cloudy            Partly cloudy throughout the day.   fog   \n",
       "3  Rain, Partially cloudy  Partly cloudy throughout the day with rain.  rain   \n",
       "4        Partially cloudy                   Clearing in the afternoon.   fog   \n",
       "\n",
       "                              stations  \n",
       "0         42182099999,VIDP,42181099999  \n",
       "1  42182099999,VIDP,42181099999,remote  \n",
       "2  42182099999,VIDP,42181099999,remote  \n",
       "3         42182099999,VIDP,42181099999  \n",
       "4  42182099999,VIDP,42181099999,remote  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "weather_1 = pd.read_csv('Delhi, India 2015-01-11 to 2016-06-30.csv')\n",
    "weather_2 = pd.read_csv('delhi 2016-07-01 to 2017-12-31.csv')\n",
    "\n",
    "# Merge (concatenate) both datasets\n",
    "weather_merged = pd.concat([weather_1, weather_2], ignore_index=True)\n",
    "\n",
    "# Convert date column to datetime\n",
    "weather_merged['datetime'] = pd.to_datetime(weather_merged['datetime'])\n",
    "\n",
    "# Sort by date just in case\n",
    "weather_merged = weather_merged.sort_values('datetime')\n",
    "\n",
    "# Check output\n",
    "weather_merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8565df-3666-4972-9b5f-57a42d4fb4d7",
   "metadata": {},
   "source": [
    "## STEP 3: Add EVENT DATA\n",
    "\n",
    "manually creating a DataFrame for key event dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8340b316-f548-443f-9e4b-8b08c1f05c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define public holidays and events in Delhi\n",
    "events = {\n",
    "    'DateTime': [\n",
    "        '2015-08-15 00:00:00',  # Independence Day\n",
    "        '2015-10-02 00:00:00',  # Gandhi Jayanti\n",
    "        '2016-01-26 00:00:00',  # Republic Day\n",
    "        '2016-08-15 00:00:00',  # Independence Day\n",
    "        '2016-10-30 00:00:00',  # Diwali\n",
    "        '2017-08-15 00:00:00',  # Independence Day\n",
    "        '2017-10-02 00:00:00',  # Gandhi Jayanti\n",
    "    ],\n",
    "    'IsEvent': [1]*7  # Binary indicator\n",
    "}\n",
    "\n",
    "event_df = pd.DataFrame(events)\n",
    "event_df['DateTime'] = pd.to_datetime(event_df['DateTime'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be61f58-936d-4e9a-9397-492206f6f7f4",
   "metadata": {},
   "source": [
    "## STEP 4: Merge All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "64996938-32fd-4aa3-b29a-138eee35fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔗 Merge traffic and weather datasets on their datetime columns\n",
    "\n",
    "# 'DateTime' from traffic and 'datetime' from weather_merged\n",
    "merged = pd.merge(traffic, weather_merged, left_on='DateTime', right_on='datetime', how='left')\n",
    "\n",
    "# 📅 Merge the result with event data to add public holiday/event indicators\n",
    "# This uses 'DateTime' as the common key\n",
    "final_merged = pd.merge(merged, event_df, on='DateTime', how='left')\n",
    "\n",
    "# 🧹 Fill missing event indicators with 0 (i.e., non-event days)\n",
    "final_merged['IsEvent'] = final_merged['IsEvent'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742ac85-4d05-445f-989e-23cb370cb0b8",
   "metadata": {},
   "source": [
    "## STEP 5: Handle Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "53b85bf3-a779-4543-afc9-2bd90f150efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📉 Check how many missing values exist in each column of the final merged dataset\n",
    "missing_summary = final_merged.isnull().sum()\n",
    "\n",
    "# 🧹 Remove any duplicate rows to ensure data integrity\n",
    "final_merged = final_merged.drop_duplicates()\n",
    "\n",
    "# 🔄 Fill missing values using forward fill (ffill)\n",
    "# This method carries forward the last known value, useful for weather data continuity\n",
    "final_merged = final_merged.ffill()\n",
    "\n",
    "\n",
    "# 📐 Normalize selected numeric features to a 0–1 range using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 🔧 Specify which columns to scale (adjust based on your actual column names)\n",
    "cols_to_scale = ['temp', 'humidity', 'windspeed', 'Vehicles']  # Example\n",
    "\n",
    "# 🧪 Apply scaling to the selected columns\n",
    "final_merged[cols_to_scale] = scaler.fit_transform(final_merged[cols_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "76db6fea-79aa-4cef-897c-35937c8db9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DateTime', 'Junction', 'Vehicles', 'ID', 'name', 'datetime', 'tempmax', 'tempmin', 'temp', 'feelslikemax', 'feelslikemin', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'precipcover', 'preciptype', 'snow', 'snowdepth', 'windgust', 'windspeed', 'winddir', 'sealevelpressure', 'cloudcover', 'visibility', 'solarradiation', 'solarenergy', 'uvindex', 'severerisk', 'sunrise', 'sunset', 'moonphase', 'conditions', 'description', 'icon', 'stations', 'IsEvent']\n"
     ]
    }
   ],
   "source": [
    "print(final_merged.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c0c06b32-4270-427a-9374-595158fce8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Integrated Dataset\n",
    "final_merged.to_csv('Final_Integrated_Dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506de12e-ec63-45a6-b774-0195a3efe0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
